{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIT License\n",
    "#\n",
    "# Copyright (C) The Adversarial Robustness Toolbox (ART) Authors 2020\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n",
    "# documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the\n",
    "# rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit\n",
    "# persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "#\n",
    "# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the\n",
    "# Software.\n",
    "#\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE\n",
    "# WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n",
    "# TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "# SOFTWARE.\n",
    "\"\"\"\n",
    "This module implements Backdoor Attacks to poison data used in ML models.\n",
    "\"\"\"\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import logging\n",
    "from typing import Callable, List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from art.attacks.evasion.projected_gradient_descent.projected_gradient_descent import ProjectedGradientDescent\n",
    "\n",
    "import import_ipynb\n",
    "from backdoor_attack_setup import PoisoningAttackBlackBox\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class BackdoorAttack(PoisoningAttackBlackBox):\n",
    "    \"\"\"\n",
    "    As talked about in Gu, et. al. 2017, this class implements the backdoor attack\n",
    "    \"\"\"\n",
    "\n",
    "    attack_params = PoisoningAttackBlackBox.attack_params + [\"perturbation\"]\n",
    "    _estimator_requirements = ()\n",
    "\n",
    "    def __init__(self, perturbation: Union[Callable, List[Callable]]) -> None:\n",
    "       \n",
    "        super().__init__()\n",
    "        self.perturbation = perturbation\n",
    "        self._check_params()\n",
    "\n",
    "    def poison(\n",
    "        self, to_be_attacked: np.ndarray, attack_target_labels: Optional[np.ndarray] = None, broadcast=False, **kwargs\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \n",
    "        if broadcast:\n",
    "            attack_labels = np.broadcast_to(attack_target_labels, (to_be_attacked.shape[0], attack_target_labels.shape[0]))\n",
    "        else:\n",
    "            attack_labels = np.copy(attack_target_labels)\n",
    "\n",
    "        number_attacked = len(to_be_attacked)\n",
    "        \n",
    "        attacked = np.copy(to_be_attacked)\n",
    "\n",
    "        if callable(self.perturbation):\n",
    "            return self.perturbation(attacked), attack_labels\n",
    "\n",
    "        for perturb in self.perturbation:\n",
    "            attacked = perturb(attacked)\n",
    "\n",
    "        return attacked, attack_labels\n",
    "\n",
    "    def _check_params(self) -> None:\n",
    "        if not (callable(self.perturbation) or all((callable(perturb) for perturb in self.perturbation))):\n",
    "            raise ValueError(\"Perturbation must be a function or a list of functions.\")\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "#Part of extension\n",
    "class PoisoningAttackCleanLabelBackdoor(PoisoningAttackBlackBox):\n",
    "    \"\"\"\n",
    "    Implementation of Clean-Label Backdoor Attacks introduced in Gu, et. al. 2017\n",
    "    Applies a number of backdoor perturbation functions and switches label to target label\n",
    "    | Paper link: https://arxiv.org/abs/1708.06733\n",
    "    \"\"\"\n",
    "\n",
    "    attack_params = PoisoningAttackBlackBox.attack_params + [\n",
    "        \"backdoor\",\n",
    "        \"proxy_classifier\",\n",
    "        \"target\",\n",
    "        \"pp_poison\",\n",
    "        \"norm\",\n",
    "        \"eps\",\n",
    "        \"eps_step\",\n",
    "        \"max_iter\",\n",
    "        \"num_random_init\",\n",
    "    ]\n",
    "    _estimator_requirements = ()\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        backdoor: BackdoorAttack,\n",
    "        proxy_classifier: \"CLASSIFIER_LOSS_GRADIENTS_TYPE\",\n",
    "        target: np.ndarray,\n",
    "        pp_poison: float = 0.33,\n",
    "        norm: Union[int, float, str] = np.inf,\n",
    "        eps: float = 0.3,\n",
    "        eps_step: float = 0.1,\n",
    "        max_iter: int = 100,\n",
    "        num_random_init: int = 0,\n",
    "    ) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "        self.backdoor = backdoor\n",
    "        self.proxy_classifier = proxy_classifier\n",
    "        self.target = target\n",
    "        self.pp_poison = pp_poison\n",
    "        self.attack = ProjectedGradientDescent(\n",
    "            proxy_classifier,\n",
    "            norm=norm,\n",
    "            eps=eps,\n",
    "            eps_step=eps_step,\n",
    "            max_iter=max_iter,\n",
    "            targeted=False,\n",
    "            num_random_init=num_random_init,\n",
    "        )\n",
    "        self._check_params()\n",
    "\n",
    "    def poison(\n",
    "        self, to_be_attacked: np.ndarray, attack_target_labels: Optional[np.ndarray] = None, broadcast=False, **kwargs\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \n",
    "        data = np.copy(to_be_attacked)\n",
    "        estimated_labels = self.proxy_classifier.predict(data) if attack_target_labels is None else np.copy(attack_target_labels)\n",
    "\n",
    "        all_indices = np.arange(len(data))\n",
    "        target_indices = all_indices[np.all(estimated_labels == self.target, axis=1)]\n",
    "        num_poison = int(self.pp_poison * len(target_indices))\n",
    "        selected_indices = np.random.choice(target_indices, num_poison)\n",
    "\n",
    "        perturbed_input = self.attack.generate(data[selected_indices])\n",
    "\n",
    "        poisoned_input, _ = self.backdoor.poison(perturbed_input, self.target, broadcast=True)\n",
    "        data[selected_indices] = poisoned_input\n",
    "\n",
    "        return data, estimated_labels\n",
    "\n",
    "    def _check_params(self) -> None:\n",
    "        if not isinstance(self.backdoor, BackdoorAttack):\n",
    "            raise ValueError(\"Backdoor must be of type BackdoorAttack\")\n",
    "        if not isinstance(self.attack, ProjectedGradientDescent):\n",
    "            raise ValueError(\"There was an issue creating the PGD attack\")\n",
    "        if not 0 < self.pp_poison < 1:\n",
    "            raise ValueError(\"pp_poison must be between 0 and 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
